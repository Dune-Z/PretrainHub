model:
  vocab_size: 32000
  hidden_size: 512
  intermediate_size: 2048
  num_hidden_layers: 4
  num_attention_heads: 16
  num_key_value_heads: 16
  max_position_embeddings: 4096
  _attn_implementation: flash_attention_2
  bos_token_id: 1
  eos_token_id: 2
  torch_dtype: bfloat16
optimizer:
  type: muon
  momentum: 0.98
  learning_rate: 4e-3
  weight_decay: 0.1
  adam_beta1: 0.8
  adam_beta2: 0.98
  adam_epsilon: 1e-15
  warmup_steps: 0
  max_steps: 200000
  decay_steps: 200000
  decay_type: linear
trainer:
  output_dir: checkpoints/50M-Muon-LR4e-3-WM0-STEP200000-BZ256-SEQ4096
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2
  learning_rate: 2e-3
  num_train_epochs: 1
  max_steps: 200000
  warmup_steps: 0
  logging_steps: 1
  save_steps: 100
  seed: 42
  bf16: true
  report_to: wandb
  run_name: 50M-Muon-LR4e-3-WM0-STEP200000-BZ256-SEQ4096
  resume_from_checkpoint: null
task:
  tokenizer_name: meta-llama/Llama-2-7b
  sequence_length: 4096
  dataset:
    path: mlfoundations/dclm-baseline-1.0
    split: train
    streaming: true